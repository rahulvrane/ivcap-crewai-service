{
  "$schema": "urn:sd-core:schema.crewai.request.1",
  "name": "Software Discovery Test - PyTorch Training Infrastructure",
  "crew-ref": "urn:sd:crewai:crew.software_discovery",
  "inputs": {
    "research_topic": "Deep Learning Training Infrastructure and MLOps Tools",
    "keywords": "PyTorch, distributed training, GPU clusters, experiment tracking, model versioning, checkpoint management, hyperparameter optimization, mixed precision training, gradient accumulation",
    "additional_information": "Research group focusing on large transformer models (1B+ parameters). Key requirements: Multi-node distributed training support with NCCL backend, integration with experiment tracking platforms (Weights & Biases, MLflow, TensorBoard), automatic checkpoint management and model versioning, scalable to 8+ GPU clusters across multiple nodes, support for mixed precision training (FP16/BF16), gradient accumulation capabilities, preferably open-source with permissive licensing (Apache 2.0 or MIT). Budget: $10k/year for compute infrastructure, prefer free or low-cost tools for experimentation. Team size: 5 ML researchers with varying experience levels. Current technology stack: PyTorch 2.0+, Python 3.10+, CUDA 11.8+, Linux Ubuntu 22.04. Use cases: Pre-training language models, fine-tuning on domain-specific datasets, hyperparameter search experiments."
  }
}

